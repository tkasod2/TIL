<h2 id="-">목차</h2>
<ul>
<li><a href="#ai개요">AI 개요</a></li>
<li><a href="#ml-vs-dl">ML vs DL</a></li>
<li><a href="#machine-learning">Machine_Learning</a><ul>
<li><a href="#machine-learning-핵심개념">Machine Learning 핵심개념</a><ul>
<li>Overfitting and Generalization</li>
<li>Gradient Descent Algorithm</li>
<li>종류</li>
</ul>
</li>
</ul>
</li>
<li><a href="#deep-learning">Deep Learning</a><ul>
<li><a href="#deep-learning-핵심개념">Deep Learning 핵심개념</a><ul>
<li>Activation Function</li>
<li>모델 구분</li>
</ul>
</li>
<li><a href="#deep-learning-model-최적화-개념">Deep Learning Model 최적화 개념</a><ul>
<li>Neural Network Optimization<ul>
<li>Weight Initialization</li>
<li>Weight Regularization</li>
<li>Advanced gradient descent algorithms(HP 관련 내용들)</li>
</ul>
</li>
<li>Avoid Overfitting<ul>
<li>Drop out</li>
<li>Batch Normalization</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="ai-">AI개요</h2>
<ul>
<li>AI란? @@데이터를 통해, @@모델을 만들고, @@기능을 만든다<ul>
<li>모델 : 데이터에 대한 설명 방법</li>
<li>종류 :<blockquote>
<p>Artificial (????) Intellgince<br>ANI (Narrow) : 약한 AI, 특정분야에서만<br>AGI (General) : 강한 AI, 인간 만큼의 지능<br>ASI (super) : 인간을 뛰어 넘는 지능.</p>
</blockquote>
</li>
<li>중요 키워드 : Auto ML</li>
<li>데이터 종류 : 정형 / 반정형(log, Sensor, )/ 비정형(이미지, 비디오, 사운드, Document)<ul>
<li><strong>그래봤자, 결국 정형데이터 처리 기법으로 처리한다.</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="ml-vs-dl">ML vs DL</h2>
<p><img src="https://miro.medium.com/max/724/1*85gM03JgUpDEQO8JKl9VpA.png" alt="image"></p>
<p><img src="https://ars.els-cdn.com/content/image/1-s2.0-S1350946218300119-gr4.jpg" alt="image"></p>
<ul>
<li>ML : 사람이 Feautre를 선정하고, Engineering을 한 데이터로 학습을 진행한다.</li>
<li>DL : Input Data에 대한 Feature Extraction과 분석과정이 모델(NN)안에서 진행된다.</li>
<li><p><em>즉, 데이터 input에서의 가공 여부가 가장 큰 차이점</em></p>
</li>
<li><p>c.f) 직무에 대한 소개</p>
<ul>
<li>Data Engineer <ul>
<li>Data Pipeline을 구축하는 업무</li>
<li>바로 분석을 할 수 있게 데이터를 전달해주는 역할</li>
</ul>
</li>
<li>Business Analyst(Data Analyst)<ul>
<li>산업공학 기반</li>
<li>모델링보다는 시각화를 통해 의사결정을 돕는 업무</li>
<li>태블로, POWER BI</li>
</ul>
</li>
<li>Machine Learning Researcher(Data Scientist)<ul>
<li>연구가 메인</li>
<li>이론과 응용측면으로 나뉨</li>
<li>학력을 굉장히 많이 따짐</li>
</ul>
</li>
</ul>
</li>
<li>c.f) 공부 소스
<a href="https://developers.google.com/machine-learning/crash-course/ml-intro">머신러닝 단기집중과정(15hr)</a>
<a href="https://www.youtube.com/watch?v=auKdde7Anr8&amp;list=PLlMkM4tgfjnJhhd4wn5aj8fVTYJwIpWkS">일주일에 논문 한개씩(PR12)</a></li>
</ul>
<h2 id="machine-learning">Machine Learning</h2>
<h3 id="machine-learning-">Machine Learning 핵심개념</h3>
<ul>
<li>정의 : 어떠한 과제 <strong>T</strong>ask는, <strong>P</strong>erformance measure 평가받고, <strong>E</strong>xperience를 통해 학습하는 프로그램.</li>
<li><p>분류</p>
<ul>
<li>Supervised Learning : 정답을 예측. classification, regression</li>
<li>Un-supervised Learning : 규칙성 찾기</li>
<li>Reinforcement Learning : 1)<strong>상태</strong>와 2)<strong>행동</strong> 사이의 상호작용을 통해, 3)환경으로부터 받는 <strong>보상</strong>을 최대화 하기 위한 4) <strong>행동정책</strong>을 찾는 알고리즘</li>
</ul>
</li>
<li><p>Data Split @ sklearn.model_selection.train_test_split</p>
<ul>
<li>Train data : Learning 목적</li>
<li>Validation data : Hyper Params tuning, <strong>Model Select</strong> 목적</li>
<li>Test data : Evaluate 목적</li>
</ul>
</li>
<li><p>Overfitting and Generalization</p>
<ul>
<li>Capacity 극대화 (즉, Feature;차원이 너무 큰 경우)</li>
<li>➡ Overfitting 발생</li>
<li>➡ Generalization Error 증가 (즉, test loss 커짐)</li>
<li>➡ New data에 대한 대응력 부족</li>
<li>➡ 대응 방법 : Data 추가확보, Regularization(L1, L2), Drop-out &amp; Batch Normalizaion; BN (딥러닝에서.), HPO, C.V.<ul>
<li>Cross Validation (교차검증)<ul>
<li>(Stratified;층화적) K-Fold CV<pre><code>  - Test Data를 일정 비율 떼고,
  - Train + Validation Data를 K등분
  - 한번씩 돌아가면서 K phase만큼 학습하고, <span class="hljs-comment">(**Phase마다 모델을 초기화됨.**)</span>
</code></pre><ul>
<li>K 번을 반복하고 평균값을 사용</li>
<li>목적은, <strong>평가</strong>시에 Train Data에 Overfitting되는것을 방지하는게 첫번째, Hyper Parameter Tuning이 두번째</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Gradient Descent Algorithm (경사하강법)</p>
<ul>
<li><strong>cost fn vs loss fn vs objective fn????</strong></li>
<li>Cost Function : 가장 적합한 Parameter $\theta$ 를 찾기위해,  $\hat{y}$ 와, $y$의 차이를 기반으로 모델 성능 지표를 찾는걸 도와주는 함수</li>
<li>cost func의 Gradient(기울기)를 기반으로 어느방향으로 이동하면 cost값을 최소화할 수 있는지 찾아가는 방식<blockquote>
<p>의문? 미분계수 0인걸 비교하면 되지 않나?<br>컴퓨터가 미분계수를 구하는 계산하는것이 어렵고, 비선형함수나, 다중회귀식이나, 아예 닫힌상태가 아닌경우에는 너무 빡셈</p>
</blockquote>
</li>
<li>MSE : 평가지표, 대표적인 Cost Function<ul>
<li>${x_i}$ : i번째, 관측치. $\hat{x_i}$ : i 번째 예측치(회귀를 통한 예측)</li>
<li>${x_i}$-$\hat{x<em>i}$ = $error</em>{i}$</li>
<li>$\frac{\sum<em>{i = 1}^{...} error</em>{i}^{2}}{n}$ : MSE</li>
<li>해당 Cost Function의 최소화를 시키는 Parmeter ${\theta}$를 찾는것.</li>
</ul>
</li>
</ul>
</li>
<li><p>종류</p>
<ul>
<li>Supervised Learning (지도학습)</li>
<li>Unsupervised Learning (비지도학습)</li>
<li>Reinforcement Learning (강화학습)</li>
</ul>
</li>
</ul>
<h2 id="deep-learning">Deep Learning</h2>
<h3 id="deep-learning-">Deep Learning 핵심개념</h3>
<ul>
<li>목차<ul>
<li>Activation Function</li>
<li>모델 구분</li>
</ul>
</li>
</ul>
<p><img src="https://miro.medium.com/max/639/1*_Epn1FopggsgvwgyDA4o8w.png" alt="image"></p>
<ul>
<li>Activation Function<ul>
<li>정의 : 이전레이어의 가중합을 받아 출력값을 생성해, 다음 레이어로 전달하는 <strong>비선형 함수</strong></li>
<li>왜 쓰는가? weight와 sum을 통한 NN 모델은 결국 <strong>선형결합(Linear combination)</strong></li>
<li>그럼, 결국 비선형의 패턴을 가진 데이터에대해서는 설명이 불가능함</li>
<li>따라서, 비선형 함수인 Activation Function을 통과시켜서 데이터에 대한 이해도를 높이는거</li>
</ul>
</li>
<li>모델의 구분<ul>
<li>MLP, SLP (Mulit/Single Layer Perceptron)<ul>
<li>SLP : AND, OR에 대한 설명만 가능. XOR에 대한 설명이 불가능</li>
<li>MLP : XOR(두 데이터의 값이 동일하면 0, 같지않으면 1을 뱉어내는 판별)의 설명을 위해 나타남.</li>
</ul>
</li>
<li>ANN<ul>
<li>Forward Propagation (Feedforward NN) : costfunction의 cost를 낮추도록 Gradient Descent를 적용하여, 원하는 결과를 얻어내기위한 적절한 $\theta$ 를 구하는 방식<ul>
<li>Layer의 복잡도가 커질수록, 연산이 너무 복잡해짐!</li>
<li>과거데이터(input 단에 가까운애들)에 대한 기억이 상실될 가능성 큼.</li>
</ul>
</li>
<li>Back Propagation : Forward 학습을 한번 하고, <strong>error</strong>(학습된 출력값 - 실제값)를 계산하여 역방향으로 전파하는 알고리즘<ul>
<li>틀린 정도(error)의 gradient(기울기)값을 역으로 계산해나가면서, G.D를 적용하고 적절한 $\theta$ 를 갱신하게함<ul>
<li>Gradient Vanishing / Gradient Explode</li>
</ul>
</li>
</ul>
</li>
<li><a href="http://playground.tensorflow.org/">http://playground.tensorflow.org/</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="deep-learning-model-">Deep Learning Model 최적화 개념</h3>
<ul>
<li>Neural Network Optimization<ul>
<li>Weight Initialization<ul>
<li>Perceptron의 선형결합(Linear Combination)의 결과값이 너무 커지거나 작아지지 않게 초기값을 설정
Gradient/Exploding Vanishing을 줄일 수 있음</li>
<li>종류<ul>
<li>Xavier Initialization (자비에/세이비어/이그제비어 초기화)<ul>
<li>Activation function이 sigmoid나 tanh일때 적용</li>
<li>딥러닝 라이브러리들의 Default임</li>
<li>표준편차가 $\sqrt{\frac{2}{n<em>{in}+n</em>{out}}}$ 인 정규분포를 따르게 가중치 초기화<blockquote>
<p>$n<em>{in}$ : 이전 layer의 퍼셉트론 수<br>$n</em>{out}$ : 현재 layer의 퍼셉트론 수</p>
</blockquote>
</li>
<li>ReLU함수에서 사용 시 출력 값이 0으로 수렴하게 되는 현상 발생</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<pre><code>        -<span class="ruby"> He Initialzation (헤/흐어 초기화)
</span>            -<span class="ruby"> Activation function이 ReLU함수일때 적용
</span>            -<span class="ruby"> 표준편차가 $\sqrt{\frac{<span class="hljs-number">2</span>}{n<span class="hljs-number">_</span>{<span class="hljs-keyword">in</span>}}}$ 인 정규분포를 따르게 가중치 초기화 
</span>
-<span class="ruby"> Weight Regularization
</span>    -<span class="ruby"> Train data만 고려된 Cost Function으로 Gradient Descent를 하면 Overfitting 위험도가 큼
</span>    -<span class="ruby"> 해서, $\theta$ 에 대한, 함수를 추가함
</span>    ![image](https://www.oreilly.com/library/view/hands-on-machine-learning/9781788393485/assets/320843d0-3683-4422-80b2-c2913f8d02d4.png)
    -<span class="ruby"> 간단한 예로는,아래 예시로 표현할 수 있음
</span>    $$ J(\theta)= MSE+ \lambda*R(\theta) $$
    $*\lambda$ : 정규화율(Regularization Rate)
    -<span class="ruby"> 모델 복합도↑ → overfitting → MSE↓ → Regularization Term ↑
</span>    -<span class="ruby"> 위 Trade-off 속에서 최적값을 찾아냄
</span>    -<span class="ruby"> L1, L2
</span>
    ![image](https://miro.medium.com/max/450/1*-LydhQEDyg-4yy5hGEj5wA.png)

    -<span class="ruby"> 아래 그림처럼, $\theta$의 변화를 통해서, 가장 낮은 MSE를 찾는(그림상에서 접점을 찾는) 방식으로 parameter가 결정됨
</span>&lt;img src="weight_regularization.png" width="80%" height="80%"/&gt;
    -<span class="ruby"> $*\lambda$ (Regularization Rate) : 스칼라값으로, 정규화 함수의 상대적 중요도
</span>
-<span class="ruby"> Advanced gradient descent algorithms
</span>    -<span class="ruby"> 세미나내용 참고(<span class="hljs-symbol">https:</span>/<span class="hljs-regexp">/github.com/tkasod</span>2/TIL/blob/main/Theory/Seminar.md)
</span>    -<span class="ruby"> learning rate
</span>    -<span class="ruby"> Batch Size
</span>    -<span class="ruby"> Optimizaer
</span>    ![image](https://image.slidesharecdn.com/random-170910154045/95/-49-638.jpg?cb=1505089848)
</code></pre><ul>
<li>Avoid Overfitting<ul>
<li>Drop out<ul>
<li>일부 Perceptron을 꺼서, 편향을 막아줌</li>
<li>앙상블 효과가 있음</li>
</ul>
</li>
<li><a href="https://github.com/tkasod2/TIL/blob/main/Theory/Seminar.md">Batch normalization</a><ul>
<li>scale에 의해 특정 params에 집중되는걸 막아주는 효과</li>
<li>최적 cost value로 접근하는데 꼭 필요함</li>
<li>일반적으로 activation function 진행전에 batch normalization을 진행하는데 이에대해서는 아직 정설은 없음(실험중)</li>
<li>장점 : <strong>학습속도</strong>&amp; 학습 시간 개선, 초기값에 영향 적음, overfitting이 잘 일어나지않음</li>
</ul>
</li>
</ul>
</li>
</ul>
