## ë”¥ëŸ¬ë‹ì—ì„œì˜ Hyper Parameterì— ëŒ€í•˜ì—¬

- Batch Normalization ; BN
    <details markdown="1">
    <summary>ì¶œì²˜</summary>  

    + ì¶œì²˜1(Batch Normalizaion) : https://arxiv.org/pdf/1502.03167.pdf
    + ì¶œì²˜2(Batch ì •ê·œí™”) : https://eehoeskrap.tistory.com/430
    </details>

    - BN ë°°ê²½
        - Gradient Exploding / Vanishing : param'sì˜ ë³€í™”ì— ë”°ë¥¸ output ë³€í™”ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ì‹ ê²½ë§ì—ì„œ, í•´ë‹¹ ì´ìŠˆë¡œ ì¸í•´ Errorê°€ í°ìƒíƒœë¡œ ìˆ˜ë ´í•˜ê²Œë¨.
        - íŠ¹íˆ, Sigmoid, Tanh ë“±ì˜ í™œì„±í™” í•¨ìˆ˜ì—ì„œ ì¶œë ¥ê°’ì˜ ë²”ìœ„ê°€ êµ‰ì¥íˆ ì¢ì•„ì§€ëŠ”ë°, (sigmoid ê²½ìš° [0,1]) Hidden Layer ì¤‘ì— ì´ëŸ¬í•œ ë¹„ì„ í˜•ì„± ë ˆì´ì–´ê°€ ì„ì–´ë“¤ì–´ê°€ê²Œë˜ë©´ ê²°êµ­ í•™ìŠµì´ ì œëŒ€ë¡œ ë˜ì§€ ì•Šê²Œë¨.
        - ì´ì— ëŒ€ì‘í•˜ëŠ” **ì§ì ‘ì ì¸ ë°©ë²•ìœ¼ë¡œì¨ BNì´ ì¶œí˜„í•˜ê²Œë¨**
    - ì™œ BN?
        - Whitening (= Standard Generalization)ì˜ í•œê³„
            - covariance matrix ê³„ì‚°, inverse matrix ê³„ì‚°ì´ ë„ˆë¬´ ë§ìŒ
            - biasì˜ ì˜í–¥ë ¥ì´ ì‚¬ë¼ì§€ê²Œë¨
            - Backpropagationì´ ë¬´ì‹œë˜ê³  íŠ¹ì • íŒŒë¼ë¯¸í„°ë§Œ ë¬´ì§€ ì»¤ì§€ê²Œë¨
        - í•™ìŠµ ì‹œ í‰ê· ê³¼ ë¶„ì‚°ì˜ ì¡°ì • ê³¼ì •ì´ ì‹ ê²½ë§ ì•ˆì— í¬í•¨
        ![image](batch_normalization.png)
        - Gradient Vanishingê³¼ Explodingì˜ ì›ì¸ì€ scale ë¬¸ì œ.
            - BN ì‚¬ìš©í•˜ë©´ ì´ì— ëŒ€í•œ ì˜í–¥ì´ ê·¹ë„ë¡œ ì‘ì•„ì§
            - BN ì‚¬ìš©í•˜ë©´ Regularization íš¨ê³¼ê°€ ìˆê¸°ì— dropout ì•ˆì¨ë„ ë¨
            



- Batch Size 
    <details markdown="1">
    <summary>ì¶œì²˜</summary>  

    + ì¶œì²˜1(ì»´í“¨í„° ë¹„ì „ ê´€ì  Batchsize) : https://deep-learning-study.tistory.com/647
    + ì¶œì²˜2(ëŸ¬ë‹ë ˆì´íŠ¸& ë°°ì¹˜ì‚¬ì´ì¦ˆ) : https://honeyjamtech.tistory.com/43
    + ì¶œì²˜3(ëŸ¬ë‹ë ˆì´íŠ¸& ë°°ì¹˜ì‚¬ì´ì¦ˆ) : https://inhovation97.tistory.com/32
    </details>

    - í¬ë©´? Noise ê°ì†Œ(Batch ë¡œ ë¶€í„° Normalizationì„ í•˜ê¸°ë•Œë¬¸), í•™ìŠµì†ë„ ë¹ ë¦„ , Overfitting ìœ„í—˜
    - ì‘ìœ¼ë©´? Noise ì¦ê°€, regularization íš¨ê³¼, stepì´ ê¸¸ì–´ì§. local minimaë¡œ ë¹ ì§ˆ ìœ„í—˜
    - ê·¸ëŸ¬ë©´ ì–´ì©Œë¼ê³ ...?ã… ã… 
        - 32~ 128 ì¶”ì²œ. [Rethinking "Batch" in BatchNorm](https://arxiv.org/pdf/2105.07576.pdf) 2021. Facebook AI Research. Yuxin Wu, Justin Johnson
            > [page6, 4-1 ë°œì·Œ] In this experiment, the best validation error is found at a normalization batch size of 32âˆ¼128, where the amount of noise and inconsistency provides balanced regularization.
        - 32 ì´ìƒí•˜ë©´ ë§ë¦¬ë¼ëŠ” ...;;
        ![image](Yann_LeCun.png)
    - **ê²°êµ­ Learning Rateì™€ ì¡°ì ˆì´ ê°™ì´ ë˜ì–´ì•¼í•¨.**

- Learning Rate
    <details markdown="1">
    <summary>ì¶œì²˜</summary>  

    + ì¶œì²˜1(PyTorchê°€ ì œê³µí•˜ëŠ” Learning rate scheduler ì •ë¦¬) : https://sanghyu.tistory.com/113
    </details>
    
    - í¬ë©´? Overshooting í˜„ìƒìœ¼ë¡œ loss ë°œì‚°ì˜ ìœ„í—˜
    - ì‘ìœ¼ë©´? Local Minimaì— ë¹ ì§ˆ ìœ„í—˜
    - ê·¸ëŸ¬ë©´? Learning Rateë¥¼ ì ì°¨ ì¤„ì—¬ì„œ í•™ìŠµì‹œí‚¤ëŠ” ê¸°ë²• í™œìš©
    - ë‹¨, ì²« lrì— ëŒ€í•œ ì™„ë²½í•œ ë°©ë²•ì€ ì—†ì„ë“¯.
        - LambdaLR í™œìš© epochë§ˆë‹¤ ê³±í•´ì£¼ëŠ” ë°©ì‹
        - ì¶”ê°€ë¡œ StepLR, MultiStepLR, ExponentialLR, ReduceLROnPlateau ë“±ì´ ìˆìŒ.
        > ### pytorch code ì˜ˆì‹œ(1)
        > optimizer = torch.optim.SGD(model.parameters(), lr=0.001)
        > scheduler = optim.lr_scheduler.LambdaLR(optimizer=optimizer,
                                lr_lambda=lambda epoch: 0.95 ** epoch)
        
        > ### pytorch code ì˜ˆì‹œ(2)
        > lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.997)
        > lr_scheduler.step()


- Num of Layers/Nodes(;Sizes) : ğŸ’¥ì£¼ì˜ 2011ë…„ ìë£Œë¥¼ ì¶œì²˜ë¡œí•¨
    <details markdown="1">
    <summary>ì¶œì²˜</summary>
    
    + ì¶œì²˜1(https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw)

    + ì¶œì²˜2(https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/) : 
    </details>

    - *One hidden layer is sufficient for the large majority of problems.*  
    Hidden Layer ì¶”ê°€ë¡œ ì„±ëŠ¥ì´ ì˜¬ë¼ê°€ëŠ”ë²•ì€ ê±°ì˜ ì—†ë‹¤.
    - ê²½í—˜ì ìœ¼ë¡œ, Input Layerì™€ Output Layerì˜ ì¤‘ê°„ì—ì„œ Num of Nodesë¥¼ ì •í•œë‹¤.

    - Single Layers vs Multiple Layers 
        - A single-layer neural network can only be used to represent linearly separable functions.(ì„ í˜•ë¬¸ì œëŠ” Hidden Layer í•˜ë‚˜ì¼ë•Œ ì¢‹ìŒ.)
        - Since a single sufficiently large hidden layer is adequate for approximation of most functions, why would anyone ever use more?
        - ê²°ë¡  : ê·¸ë•Œê·¸ë•Œ ë‹¤ë¥´ë‹¤. 


    - ì ì ˆí•œ ê°¯ìˆ˜ì˜ Nodes ê°¯ìˆ˜ ì„¤ì •
        - I donâ€™t know. Use systematic experimentation to discover what works best for your specific dataset.
        - Steffen B Petersen(2013) ğŸ’¥ê°œì¸ì˜ê²¬ë¿ì¼ìˆ˜ë„ ìˆìŒ.
            - neurons ì„¤ì •ì€ input layerì™€ output layer ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ì„¤ì •
            - hidden node= input node *2/3 + output node
            - hidden nodesëŠ” input nodes ì˜ 2ë°°ë³´ë‹¤ ì‘ì•„ì•¼ë¨
        