## Data ì €ì¥ì†Œ (21.09.07)

<details markdown="1">


- Data ì €ì¥ì†Œ ì¢…ë¥˜ì™€ íŠ¹ì§•
    <details markdown="1">
    <summary>ì¶œì²˜</summary>

    + ì¶œì²˜1 : https://couplewith.tistory.com/entry/Bigdata-%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%9B%A8%EC%96%B4-%ED%95%98%EC%9A%B0%EC%8A%A4-DataWare-House-%EA%B7%B8%EB%A6%AC%EA%B3%A0-Data-Lake
    + ì¶œì²˜2 : https://ko.myservername.com/what-is-data-lake-data-warehouse-vs-data-lake

    + ì¶œì²˜3 : https://datalibrary.tistory.com/100

    </details>

    - Data Mart
        ![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FDf8UW%2FbtqQXBXOAsD%2F6JCIJiWMFFvWkZwFm7fGiK%2Fimg.png)
        - Data Warehouseì— ìˆëŠ” ì‘ì€ í•˜ìœ„ ì§‘í•©ìœ¼ë¡œ ì£¼ë¡œ êµ¬ì²´ì ì¸ íŠ¹ì • ë¶€ì„œë‚˜ í”„ë¡œì íŠ¸ ë“±ì˜ ì‘ì€ ë‹¨ìœ„ì˜ ë¶„ì„ì„ ìš”êµ¬í•  ë•Œì— ì‚¬ìš©
        - DWì—ì„œ ë¶„ì„ì— í•„ìš”í•œ ì •ë³´ë§Œì„ ë½‘ì•„ì„œ *ìš”ì•½ëœ ë°ì´í„°ë¡œ* êµ¬ì„±
        - DWì—†ì´ ë‹¨ë…ìœ¼ë¡œ êµ¬ì¶•í•˜ê¸°ë„í•¨

    - Data Warehouse (;DW) 
        - DB(Database)ì™€ì˜ ì°¨ì´? 
            - DBëŠ” ë³´í†µ, ì¶•ì  ë°ì´í„°ë¥¼ í†µí•œ íŠ¸ëœì­ì…˜ ì²˜ë¦¬ê°€ ëª©ì ì´ë¼ ì†Œê·œëª¨ ë°ì´í„°ì— ëŒ€í•œ ëŒ€ê·œëª¨ì˜ ì ‘ê·¼ì´ ìš©ì´í•¨
            - ë°˜ë©´, DWëŠ” **ë¹…ë°ì´í„° ë¶„ì„ì„ ìœ„í•œ ëª©ì ** ìœ¼ë¡œ, ì—¬ëŸ¬ ì¶œì²˜ì˜ ë°ì´í„°ë¥¼ ì •ì œí•˜ì—¬ ë‹´ê³  ìˆê³ , ë°ì´í„° ì²˜ë¦¬ëŸ‰ì˜ ê·¹ëŒ€í™”ë¥¼ íŠ¹ì§•ìœ¼ë¡œ ê°€ì§
        - DW ì •ì˜ : Dataê¸°ë°˜ì˜ í‰ê°€ë¥¼ ë‚´ë¦´ ìˆ˜ ìˆë„ë¡ ë¶„ì„ê°€ëŠ¥í•œ ì¤‘ì•™ Repository
        - ì¦‰ ë°˜ë³µì ì¸ ë³´ê³ , ì‘ì—… ë“±ì— í™œìš©ë˜ëŠ” ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” ì°½ê³ 
        - ì •ì œê°€ ë˜ì–´ìˆëŠ” ë°ì´í„°
        - SQL, BIë¥¼ í†µí•´ì„œ ë°ì´í„°ì— ì ‘ê·¼

    - Data Lake(*new*)
        - DWì™€ì˜ ì°¨ì´?
            ![image](https://blog.kakaocdn.net/dn/cWmHkY/btqM64UYwAQ/xDyhwYXDNSudbqXny6uc7K/img.webp)
            - ë³„ë„ì˜ ì •ì œ ê³¼ì •ì´ ì—†ì´, raw data ìƒíƒœë¡œ ì €ì¥(ì¦‰, dataì˜ Inê³¼ ë™ì‹œì— Outì´ ê°€ëŠ¥í•¨)
            - ìŠ¤í‚¤ë§ˆ(ë°ì´í„° ê¼´)ì´ ë¶„ì„í•˜ëŠ” ì‹œì ì— ì“°ì—¬ì§ (DWëŠ”, ì›¨ì–´í•˜ìš°ìŠ¤ êµ¬í˜„ë‹¨ê³„ì—ì„œ ìƒì„±ë¨)
            - ìƒëŒ€ì ìœ¼ë¡œ ìœ ì—°ì„±ì´ êµ‰ì¥íˆ ì¢‹ìŒ
</details>


## ë”¥ëŸ¬ë‹ì—ì„œì˜ Hyper Parameterì— ëŒ€í•˜ì—¬ (21.09.06)
<details markdown="1">

- Batch Normalization ; BN
    <details markdown="1">
    <summary>ì¶œì²˜</summary>  

    + ì¶œì²˜1(Batch Normalizaion) : https://arxiv.org/pdf/1502.03167.pdf
    + ì¶œì²˜2(Batch ì •ê·œí™”) : https://eehoeskrap.tistory.com/430
    </details>

    - BN ë°°ê²½
        - Gradient Exploding / Vanishing : param'sì˜ ë³€í™”ì— ë”°ë¥¸ output ë³€í™”ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ì‹ ê²½ë§ì—ì„œ, í•´ë‹¹ ì´ìŠˆë¡œ ì¸í•´ Errorê°€ í°ìƒíƒœë¡œ ìˆ˜ë ´í•˜ê²Œë¨.
        - íŠ¹íˆ, Sigmoid, Tanh ë“±ì˜ í™œì„±í™” í•¨ìˆ˜ì—ì„œ ì¶œë ¥ê°’ì˜ ë²”ìœ„ê°€ êµ‰ì¥íˆ ì¢ì•„ì§€ëŠ”ë°, (sigmoid ê²½ìš° [0,1]) Hidden Layer ì¤‘ì— ì´ëŸ¬í•œ ë¹„ì„ í˜•ì„± ë ˆì´ì–´ê°€ ì„ì–´ë“¤ì–´ê°€ê²Œë˜ë©´ ê²°êµ­ í•™ìŠµì´ ì œëŒ€ë¡œ ë˜ì§€ ì•Šê²Œë¨.
        - ì´ì— ëŒ€ì‘í•˜ëŠ” **ì§ì ‘ì ì¸ ë°©ë²•ìœ¼ë¡œì¨ BNì´ ì¶œí˜„í•˜ê²Œë¨**
    - ì™œ BN?
        - Whitening (= Standard Generalization)ì˜ í•œê³„
            - covariance matrix ê³„ì‚°, inverse matrix ê³„ì‚°ì´ ë„ˆë¬´ ë§ìŒ
            - biasì˜ ì˜í–¥ë ¥ì´ ì‚¬ë¼ì§€ê²Œë¨
            - Backpropagationì´ ë¬´ì‹œë˜ê³  íŠ¹ì • íŒŒë¼ë¯¸í„°ë§Œ ë¬´ì§€ ì»¤ì§€ê²Œë¨
        - í•™ìŠµ ì‹œ í‰ê· ê³¼ ë¶„ì‚°ì˜ ì¡°ì • ê³¼ì •ì´ ì‹ ê²½ë§ ì•ˆì— í¬í•¨
        ![image](batch_normalization.png)
        - Gradient Vanishingê³¼ Explodingì˜ ì›ì¸ì€ scale ë¬¸ì œ.
            - BN ì‚¬ìš©í•˜ë©´ ì´ì— ëŒ€í•œ ì˜í–¥ì´ ê·¹ë„ë¡œ ì‘ì•„ì§
            - BN ì‚¬ìš©í•˜ë©´ Regularization íš¨ê³¼ê°€ ìˆê¸°ì— dropout ì•ˆì¨ë„ ë¨
            
<br>

- Batch Size 
    <details markdown="1">
    <summary>ì¶œì²˜</summary>  

    + ì¶œì²˜1(ì»´í“¨í„° ë¹„ì „ ê´€ì  Batchsize) : https://deep-learning-study.tistory.com/647
    + ì¶œì²˜2(ëŸ¬ë‹ë ˆì´íŠ¸& ë°°ì¹˜ì‚¬ì´ì¦ˆ) : https://honeyjamtech.tistory.com/43
    + ì¶œì²˜3(ëŸ¬ë‹ë ˆì´íŠ¸& ë°°ì¹˜ì‚¬ì´ì¦ˆ) : https://inhovation97.tistory.com/32
    </details>

    - í¬ë©´? Noise ê°ì†Œ(Batch ë¡œ ë¶€í„° Normalizationì„ í•˜ê¸°ë•Œë¬¸), í•™ìŠµì†ë„ ë¹ ë¦„ , Overfitting ìœ„í—˜
    - ì‘ìœ¼ë©´? Noise ì¦ê°€, regularization íš¨ê³¼, stepì´ ê¸¸ì–´ì§. local minimaë¡œ ë¹ ì§ˆ ìœ„í—˜
    - ê·¸ëŸ¬ë©´ ì–´ì©Œë¼ê³ ...?ã… ã… 
        - 32~ 128 ì¶”ì²œ. [Rethinking "Batch" in BatchNorm](https://arxiv.org/pdf/2105.07576.pdf) 2021. Facebook AI Research. Yuxin Wu, Justin Johnson
            > [page6, 4-1 ë°œì·Œ] In this experiment, the best validation error is found at a normalization batch size of 32âˆ¼128, where the amount of noise and inconsistency provides balanced regularization.
        - 32 ì´ìƒí•˜ë©´ ë§ë¦¬ë¼ëŠ” ...;;
        ![image](Yann_LeCun.png)
    - **ê²°êµ­ Learning Rateì™€ ì¡°ì ˆì´ ê°™ì´ ë˜ì–´ì•¼í•¨.**  

<br>

- Learning Rate
    <details markdown="1">
    <summary>ì¶œì²˜</summary>  

    + ì¶œì²˜1(PyTorchê°€ ì œê³µí•˜ëŠ” Learning rate scheduler ì •ë¦¬) : https://sanghyu.tistory.com/113
    </details>
    
    - í¬ë©´? Overshooting í˜„ìƒìœ¼ë¡œ loss ë°œì‚°ì˜ ìœ„í—˜
    - ì‘ìœ¼ë©´? Local Minimaì— ë¹ ì§ˆ ìœ„í—˜
    - ê·¸ëŸ¬ë©´? Learning Rateë¥¼ ì ì°¨ ì¤„ì—¬ì„œ í•™ìŠµì‹œí‚¤ëŠ” ê¸°ë²• í™œìš©
    - ë‹¨, ì²« lrì— ëŒ€í•œ ì™„ë²½í•œ ë°©ë²•ì€ ì—†ì„ë“¯.
        - LambdaLR í™œìš© epochë§ˆë‹¤ ê³±í•´ì£¼ëŠ” ë°©ì‹
        - ì¶”ê°€ë¡œ StepLR, MultiStepLR, ExponentialLR, ReduceLROnPlateau ë“±ì´ ìˆìŒ.
        > ### pytorch code ì˜ˆì‹œ(1)
        > optimizer = torch.optim.SGD(model.parameters(), lr=0.001)
        > scheduler = optim.lr_scheduler.LambdaLR(optimizer=optimizer,
                                lr_lambda=lambda epoch: 0.95 ** epoch)
        
        > ### pytorch code ì˜ˆì‹œ(2)
        > lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.997)
        > lr_scheduler.step()

<br>

- Num of Layers/Nodes(;Sizes) : ğŸ’¥ì£¼ì˜ 2011ë…„ ìë£Œë¥¼ ì¶œì²˜ë¡œí•¨
    <details markdown="1">
    <summary>ì¶œì²˜</summary>
    
    + ì¶œì²˜1(https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw)

    + ì¶œì²˜2(https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/) : 
    </details>

    - *One hidden layer is sufficient for the large majority of problems.*  
    Hidden Layer ì¶”ê°€ë¡œ ì„±ëŠ¥ì´ ì˜¬ë¼ê°€ëŠ”ë²•ì€ ê±°ì˜ ì—†ë‹¤.
    - ê²½í—˜ì ìœ¼ë¡œ, Input Layerì™€ Output Layerì˜ ì¤‘ê°„ì—ì„œ Num of Nodesë¥¼ ì •í•œë‹¤.

    - Single Layers vs Multiple Layers 
        - A single-layer neural network can only be used to represent linearly separable functions.(ì„ í˜•ë¬¸ì œëŠ” Hidden Layer í•˜ë‚˜ì¼ë•Œ ì¢‹ìŒ.)
        - Since a single sufficiently large hidden layer is adequate for approximation of most functions, why would anyone ever use more?
        - ê²°ë¡  : ê·¸ë•Œê·¸ë•Œ ë‹¤ë¥´ë‹¤. 


    - ì ì ˆí•œ ê°¯ìˆ˜ì˜ Nodes ê°¯ìˆ˜ ì„¤ì •
        - I donâ€™t know. Use systematic experimentation to discover what works best for your specific dataset.
        - Steffen B Petersen(2013) ğŸ’¥ê°œì¸ì˜ê²¬ë¿ì¼ìˆ˜ë„ ìˆìŒ.
            - neurons ì„¤ì •ì€ input layerì™€ output layer ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ì„¤ì •
            - hidden node= input node *2/3 + output node
            - hidden nodesëŠ” input nodes ì˜ 2ë°°ë³´ë‹¤ ì‘ì•„ì•¼ë¨

<br>
<br>

</details>