{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 목차\r\n",
    "- [AI 개요](#ai개요)\r\n",
    "- [ML vs DL](#ml-vs-dl)\r\n",
    "- [Machine_Learning](#machine-learning)\r\n",
    "    - [Machine Learning 핵심개념](#machine-learning-핵심개념)\r\n",
    "        - Overfitting and Generalization\r\n",
    "        - Gradient Descent Algorithm\r\n",
    "        - 종류\r\n",
    "- [Deep Learning](#deep-learning)\r\n",
    "    - [Deep Learning 핵심개념](#deep-learning-핵심개념)\r\n",
    "        - Activation Function\r\n",
    "        - 모델 구분\r\n",
    "    - [Deep Learning Model 최적화 개념](#deep-learning-model-최적화-개념)\r\n",
    "        - Neural Network Optimization\r\n",
    "            - Weight Initialization\r\n",
    "            - Weight Regularization\r\n",
    "            - Advanced gradient descent algorithms(HP 관련 내용들)\r\n",
    "        - Avoid Overfitting\r\n",
    "            - Drop out\r\n",
    "            - Batch Normalization\r\n",
    "\r\n",
    "## AI개요\r\n",
    "\r\n",
    "\r\n",
    "- AI란? @@데이터를 통해, @@모델을 만들고, @@기능을 만든다\r\n",
    "  - 모델 : 데이터에 대한 설명 방법\r\n",
    "  - 종류 :\r\n",
    "    > Artificial (????) Intellgince  \r\n",
    "    > ANI (Narrow) : 약한 AI, 특정분야에서만  \r\n",
    "    > AGI (General) : 강한 AI, 인간 만큼의 지능  \r\n",
    "    > ASI (super) : 인간을 뛰어 넘는 지능.\r\n",
    "  - 중요 키워드 : Auto ML\r\n",
    "  - 데이터 종류 : 정형 / 반정형(log, Sensor, )/ 비정형(이미지, 비디오, 사운드, Document)\r\n",
    "    - **그래봤자, 결국 정형데이터 처리 기법으로 처리한다.**\r\n",
    "\r\n",
    "\r\n",
    "## ML vs DL\r\n",
    " \r\n",
    "\r\n",
    "![image](https://miro.medium.com/max/724/1*85gM03JgUpDEQO8JKl9VpA.png)\r\n",
    "\r\n",
    "![image](https://ars.els-cdn.com/content/image/1-s2.0-S1350946218300119-gr4.jpg)\r\n",
    "\r\n",
    "- ML : 사람이 Feautre를 선정하고, Engineering을 한 데이터로 학습을 진행한다.\r\n",
    "- DL : Input Data에 대한 Feature Extraction과 분석과정이 모델(NN)안에서 진행된다.\r\n",
    "- *즉, 데이터 input에서의 가공 여부가 가장 큰 차이점*\r\n",
    "\r\n",
    "- c.f) 직무에 대한 소개\r\n",
    "    - Data Engineer \r\n",
    "        - Data Pipeline을 구축하는 업무\r\n",
    "        - 바로 분석을 할 수 있게 데이터를 전달해주는 역할\r\n",
    "    - Business Analyst(Data Analyst)\r\n",
    "        - 산업공학 기반\r\n",
    "        - 모델링보다는 시각화를 통해 의사결정을 돕는 업무\r\n",
    "        - 태블로, POWER BI\r\n",
    "    - Machine Learning Researcher(Data Scientist)\r\n",
    "        - 연구가 메인\r\n",
    "        - 이론과 응용측면으로 나뉨\r\n",
    "        - 학력을 굉장히 많이 따짐\r\n",
    "- c.f) 공부 소스\r\n",
    "[머신러닝 단기집중과정(15hr)](https://developers.google.com/machine-learning/crash-course/ml-intro)\r\n",
    "[일주일에 논문 한개씩(PR12)](https://www.youtube.com/watch?v=auKdde7Anr8&list=PLlMkM4tgfjnJhhd4wn5aj8fVTYJwIpWkS)\r\n",
    "\r\n",
    "\r\n",
    "## Machine Learning\r\n",
    "\r\n",
    "### Machine Learning 핵심개념\r\n",
    "\r\n",
    "- 정의 : 어떠한 과제 **T**ask는, **P**erformance measure 평가받고, **E**xperience를 통해 학습하는 프로그램.\r\n",
    "- 분류\r\n",
    "\r\n",
    "  - Supervised Learning : 정답을 예측. classification, regression\r\n",
    "  - Un-supervised Learning : 규칙성 찾기\r\n",
    "  - Reinforcement Learning : 1)**상태**와 2)**행동** 사이의 상호작용을 통해, 3)환경으로부터 받는 **보상**을 최대화 하기 위한 4) **행동정책**을 찾는 알고리즘\r\n",
    "\r\n",
    "- Data Split @ sklearn.model_selection.train_test_split\r\n",
    "  - Train data : Learning 목적\r\n",
    "  - Validation data : Hyper Params tuning, **Model Select** 목적\r\n",
    "  - Test data : Evaluate 목적\r\n",
    "\r\n",
    "- Overfitting and Generalization\r\n",
    "    - Capacity 극대화 (즉, Feature;차원이 너무 큰 경우)\r\n",
    "    - ➡ Overfitting 발생\r\n",
    "    - ➡ Generalization Error 증가 (즉, test loss 커짐)\r\n",
    "    - ➡ New data에 대한 대응력 부족\r\n",
    "    - ➡ 대응 방법 : Data 추가확보, Regularization(L1, L2), Drop-out & Batch Normalizaion; BN (딥러닝에서.), HPO, C.V.\r\n",
    "        - Cross Validation (교차검증)\r\n",
    "            - (Stratified;층화적) K-Fold CV\r\n",
    "                    - Test Data를 일정 비율 떼고,\r\n",
    "                    - Train + Validation Data를 K등분\r\n",
    "                    - 한번씩 돌아가면서 K phase만큼 학습하고, (**Phase마다 모델을 초기화됨.**)\r\n",
    "                - K 번을 반복하고 평균값을 사용\r\n",
    "                - 목적은, **평가**시에 Train Data에 Overfitting되는것을 방지하는게 첫번째, Hyper Parameter Tuning이 두번째\r\n",
    "- Gradient Descent Algorithm (경사하강법)\r\n",
    "    - **cost fn vs loss fn vs objective fn????**\r\n",
    "    - Cost Function : 가장 적합한 Parameter $\\theta$ 를 찾기위해,  $\\hat{y}$ 와, $y$의 차이를 기반으로 모델 성능 지표를 찾는걸 도와주는 함수\r\n",
    "    - cost func의 Gradient(기울기)를 기반으로 어느방향으로 이동하면 cost값을 최소화할 수 있는지 찾아가는 방식\r\n",
    "    > 의문? 미분계수 0인걸 비교하면 되지 않나?  \r\n",
    "    > 컴퓨터가 미분계수를 구하는 계산하는것이 어렵고, 비선형함수나, 다중회귀식이나, 아예 닫힌상태가 아닌경우에는 너무 빡셈\r\n",
    "    - MSE : 평가지표, 대표적인 Cost Function\r\n",
    "      - ${x_i}$ : i번째, 관측치. $\\hat{x_i}$ : i 번째 예측치(회귀를 통한 예측)\r\n",
    "      - ${x_i}$-$\\hat{x_i}$ = $error_{i}$\r\n",
    "      - $\\frac{\\sum_{i = 1}^{...} error_{i}^{2}}{n}$ : MSE\r\n",
    "      - 해당 Cost Function의 최소화를 시키는 Parmeter ${\\theta}$를 찾는것.\r\n",
    "\r\n",
    "- 종류\r\n",
    "    - Supervised Learning (지도학습)\r\n",
    "    - Unsupervised Learning (비지도학습)\r\n",
    "    - Reinforcement Learning (강화학습)\r\n",
    "    \r\n",
    "    \r\n",
    "\r\n",
    "## Deep Learning\r\n",
    "\r\n",
    "### Deep Learning 핵심개념\r\n",
    "- 목차\r\n",
    "    - Activation Function\r\n",
    "    - 모델 구분\r\n",
    "\r\n",
    "\r\n",
    "![image](https://miro.medium.com/max/639/1*_Epn1FopggsgvwgyDA4o8w.png)\r\n",
    "\r\n",
    "- Activation Function\r\n",
    "    - 정의 : 이전레이어의 가중합을 받아 출력값을 생성해, 다음 레이어로 전달하는 **비선형 함수**\r\n",
    "    - 왜 쓰는가? weight와 sum을 통한 NN 모델은 결국 **선형결합(Linear combination)**\r\n",
    "    - 그럼, 결국 비선형의 패턴을 가진 데이터에대해서는 설명이 불가능함\r\n",
    "    - 따라서, 비선형 함수인 Activation Function을 통과시켜서 데이터에 대한 이해도를 높이는거\r\n",
    "- 모델의 구분\r\n",
    "    - MLP, SLP (Mulit/Single Layer Perceptron)\r\n",
    "        - SLP : AND, OR에 대한 설명만 가능. XOR에 대한 설명이 불가능\r\n",
    "        - MLP : XOR(두 데이터의 값이 동일하면 0, 같지않으면 1을 뱉어내는 판별)의 설명을 위해 나타남.\r\n",
    "    - ANN\r\n",
    "        - Forward Propagation (Feedforward NN) : costfunction의 cost를 낮추도록 Gradient Descent를 적용하여, 원하는 결과를 얻어내기위한 적절한 $\\theta$ 를 구하는 방식\r\n",
    "            - Layer의 복잡도가 커질수록, 연산이 너무 복잡해짐!\r\n",
    "            - 과거데이터(input 단에 가까운애들)에 대한 기억이 상실될 가능성 큼.\r\n",
    "        - Back Propagation : Forward 학습을 한번 하고, **error**(학습된 출력값 - 실제값)를 계산하여 역방향으로 전파하는 알고리즘\r\n",
    "            - 틀린 정도(error)의 gradient(기울기)값을 역으로 계산해나가면서, G.D를 적용하고 적절한 $\\theta$ 를 갱신하게함\r\n",
    "                - Gradient Vanishing / Gradient Explode\r\n",
    "        - http://playground.tensorflow.org/\r\n",
    "\r\n",
    "\r\n",
    "### Deep Learning Model 최적화 개념\r\n",
    "\r\n",
    "- Neural Network Optimization\r\n",
    "    - Weight Initialization\r\n",
    "        - Perceptron의 선형결합(Linear Combination)의 결과값이 너무 커지거나 작아지지 않게 초기값을 설정\r\n",
    "        Gradient/Exploding Vanishing을 줄일 수 있음\r\n",
    "        - 종류\r\n",
    "            - Xavier Initialization (자비에/세이비어/이그제비어 초기화)\r\n",
    "                - Activation function이 sigmoid나 tanh일때 적용\r\n",
    "                - 딥러닝 라이브러리들의 Default임\r\n",
    "                - 표준편차가 $\\sqrt{\\frac{2}{n_{in}+n_{out}}}$ 인 정규분포를 따르게 가중치 초기화\r\n",
    "                    > $n_{in}$ : 이전 layer의 퍼셉트론 수  \r\n",
    "                    > $n_{out}$ : 현재 layer의 퍼셉트론 수\r\n",
    "                - ReLU함수에서 사용 시 출력 값이 0으로 수렴하게 되는 현상 발생 @@@@ 왜??\r\n",
    "\r\n",
    "\r\n",
    "            - He Initialzation (헤/흐어 초기화)\r\n",
    "                - Activation function이 ReLU함수일때 적용\r\n",
    "                - 표준편차가 $\\sqrt{\\frac{2}{n_{in}}}$ 인 정규분포를 따르게 가중치 초기화 \r\n",
    "\r\n",
    "    - Weight Regularization\r\n",
    "        - Train data만 고려된 Cost Function으로 Gradient Descent를 하면 Overfitting 위험도가 큼\r\n",
    "        - 해서, $\\theta$ 에 대한, 함수를 추가함\r\n",
    "        ![image](https://www.oreilly.com/library/view/hands-on-machine-learning/9781788393485/assets/320843d0-3683-4422-80b2-c2913f8d02d4.png)\r\n",
    "        - 간단한 예로는,아래 예시로 표현할 수 있음\r\n",
    "        $$ J(\\theta)=MSE+\\lambda*R(\\theta) $$\r\n",
    "        $*\\lambda$ : 정규화율(Regularization Rate)\r\n",
    "        - 모델 복합도↑ → overfitting → MSE↓ → Regularization Term ↑\r\n",
    "        - 위 Trade-off 속에서 최적값을 찾아냄\r\n",
    "        - L1, L2\r\n",
    "\r\n",
    "        ![image](https://miro.medium.com/max/450/1*-LydhQEDyg-4yy5hGEj5wA.png)\r\n",
    "\r\n",
    "        - 아래 그림처럼, $\\theta$의 변화를 통해서, 가장 낮은 MSE를 찾는(그림상에서 접점을 찾는) 방식으로 parameter가 결정됨\r\n",
    "    <img src=\"./image/weight_regularization.png\" width=\"80%\" height=\"80%\"/>\r\n",
    "        - $*\\lambda$ (Regularization Rate) : 스칼라값으로, 정규화 함수의 상대적 중요도\r\n",
    "        \r\n",
    "    - Advanced gradient descent algorithms\r\n",
    "        - 세미나내용 참고(https://github.com/tkasod2/TIL/blob/main/Theory/Seminar.md)\r\n",
    "        - learning rate\r\n",
    "        - Batch Size\r\n",
    "        - Optimizaer\r\n",
    "        ![image](https://image.slidesharecdn.com/random-170910154045/95/-49-638.jpg?cb=1505089848)\r\n",
    "\r\n",
    "- Avoid Overfitting\r\n",
    "    - Drop out\r\n",
    "        - 일부 Perceptron을 꺼서, 편향을 막아줌\r\n",
    "        - 앙상블 효과가 있음\r\n",
    "    - [Batch normalization](https://github.com/tkasod2/TIL/blob/main/Theory/Seminar.md)\r\n",
    "        - scale에 의해 특정 params에 집중되는걸 막아주는 효과\r\n",
    "        - 최적 cost value로 접근하는데 꼭 필요함\r\n",
    "        - 일반적으로 activation function 진행전에 batch normalization을 진행하는데 이에대해서는 아직 정설은 없음(실험중)\r\n",
    "        - 장점 : **학습속도**& 학습 시간 개선, 초기값에 영향 적음, overfitting이 잘 일어나지않음\r\n",
    "![image](./image/MLDL_기초개념5.png)\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}